{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Recommender Section: In this section, you can enter words to find movies with plots most similar to those words."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installing required packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: spacy in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.7.4)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: click in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.6.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (69.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.10.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hamid\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas nltk spacy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T16:33:45.175983Z",
     "start_time": "2024-04-01T16:33:42.253812100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T17:02:08.868915600Z",
     "start_time": "2024-04-01T17:02:04.884137100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the clean and pre-processed dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "df = pd.read_csv('movies.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T20:09:31.006486300Z",
     "start_time": "2024-04-01T20:09:30.838742700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Functions designed to harmonize the format of user-input words with the dataset's structure by applying preprocessing steps."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "abbrev_dict = {\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i'm'a\": \"i am about to\",\n",
    "    \"i'm'o\": \"i am going to\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he will have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it had\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so is\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "def expand_abbrv(text):\n",
    "    for key, value in abbrev_dict.items():\n",
    "        return text.replace(key, value)\n",
    "\n",
    "def delete_punctuation(text):\n",
    "    trans_table = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(trans_table)\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def delete_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "def stem(tokens):\n",
    "    return [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "def pre_processing_pipe(text):\n",
    "    expanded_text = expand_abbrv(text)\n",
    "    dirty_text = delete_punctuation(expanded_text)\n",
    "    tokens = tokenize(dirty_text)\n",
    "    #sw_deleted_tokens = delete_stopwords(tokens)\n",
    "    lemmatized_tokens = lemmatize(tokens)\n",
    "    return lemmatized_tokens\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T20:10:24.479048800Z",
     "start_time": "2024-04-01T20:10:24.468430400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Taking the input form user"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### for testing we enter the Se7en movie plot:\n",
    "#### Two detectives, a rookie and a veteran, hunt a serial killer who uses the seven deadly sins as his motives.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['two', 'detective', 'a', 'rookie', 'and', 'a', 'veteran', 'hunt', 'a', 'serial', 'killer', 'who', 'us', 'the', 'seven', 'deadly', 'sin', 'a', 'his', 'motif']\n"
     ]
    }
   ],
   "source": [
    "user_query = str(input()).lower()\n",
    "user_query = pre_processing_pipe(user_query)\n",
    "print(user_query)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T20:10:29.105837200Z",
     "start_time": "2024-04-01T20:10:26.843591600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## In this part I loaded tokens into a dataframe and saved it in a .csv file to have a common structure for data to compare and compute the similarity."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "tokens = [user_query]\n",
    "tokens_df = pd.DataFrame({'index': [0], 'tokens':tokens})\n",
    "tokens_df.to_csv('query_tokens.csv')\n",
    "tokens_df = pd.read_csv('query_tokens.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T20:10:40.808219100Z",
     "start_time": "2024-04-01T20:10:40.793413200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Went over the dataset and compared the input with all movie plots to compute similarity. I used the spaCy library to vectorize the tokens. Since I used NLTK in earlier steps, I created the doc object from scratch without applying the spaCy processes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13839/13839 [00:15<00:00, 882.20it/s]\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Doc\n",
    "doc = Doc(nlp.vocab, words=tokens_df.tokens[0])\n",
    "indices = []\n",
    "similarities = []\n",
    "for i in tqdm(range(df.shape[0])):\n",
    "    doc_temp = Doc(nlp.vocab, words=df.tokens[i])\n",
    "    similarity = doc.similarity(doc_temp)\n",
    "    indices.append(i)\n",
    "    similarities.append(similarity)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T20:11:06.970060400Z",
     "start_time": "2024-04-01T20:10:51.267649400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the similarities into a dataframe to preserve indices and then sort by similarity score to find the close ones."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [
    {
     "data": {
      "text/plain": "       index  similarity\n12272  12272    0.994431\n7916    7916    0.994075\n5890    5890    0.993554\n7537    7537    0.993423\n9191    9191    0.993286\n7882    7882    0.993082\n5568    5568    0.992746\n12798  12798    0.992498\n2152    2152    0.992344\n6449    6449    0.992277",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>similarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12272</th>\n      <td>12272</td>\n      <td>0.994431</td>\n    </tr>\n    <tr>\n      <th>7916</th>\n      <td>7916</td>\n      <td>0.994075</td>\n    </tr>\n    <tr>\n      <th>5890</th>\n      <td>5890</td>\n      <td>0.993554</td>\n    </tr>\n    <tr>\n      <th>7537</th>\n      <td>7537</td>\n      <td>0.993423</td>\n    </tr>\n    <tr>\n      <th>9191</th>\n      <td>9191</td>\n      <td>0.993286</td>\n    </tr>\n    <tr>\n      <th>7882</th>\n      <td>7882</td>\n      <td>0.993082</td>\n    </tr>\n    <tr>\n      <th>5568</th>\n      <td>5568</td>\n      <td>0.992746</td>\n    </tr>\n    <tr>\n      <th>12798</th>\n      <td>12798</td>\n      <td>0.992498</td>\n    </tr>\n    <tr>\n      <th>2152</th>\n      <td>2152</td>\n      <td>0.992344</td>\n    </tr>\n    <tr>\n      <th>6449</th>\n      <td>6449</td>\n      <td>0.992277</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = {'index' : indices, 'similarity' : similarities }\n",
    "sim_df = pd.DataFrame(frames)\n",
    "sim_df= sim_df.sort_values(by=['similarity'], ascending=False)\n",
    "sim_df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T20:11:09.003364Z",
     "start_time": "2024-04-01T20:11:08.972886700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 5 movies for you:\n"
     ]
    },
    {
     "data": {
      "text/plain": "name                                     The Thing About My Folks\nyear                                                         2005\nscore                                                         6.4\nvoters_count                                                 2400\nplot            bens dad sam shows up one night with a note th...\ntokens          ['ben', 'dad', 'sam', 'show', 'one', 'night', ...\nName: 12272, dtype: object"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "name                                                    Sol Goode\nyear                                                         2003\nscore                                                         5.3\nvoters_count                                                 1300\nplot            sol goode has led an easy life thanks to his c...\ntokens          ['sol', 'goode', 'led', 'easy', 'life', 'thank...\nName: 7916, dtype: object"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "name                                    I Ought to Be in Pictures\nyear                                                         1982\nscore                                                         6.1\nvoters_count                                                 1100\nplot            daughter leaves her mother from the east coast...\ntokens          ['daughter', 'leaf', 'mother', 'east', 'coast'...\nName: 5890, dtype: object"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "name                                                    Choose Me\nyear                                                         1984\nscore                                                         6.7\nvoters_count                                                 3100\nplot            love doctor radio host nancy bar owner eve pea...\ntokens          ['love', 'doctor', 'radio', 'host', 'nancy', '...\nName: 7537, dtype: object"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"we have 5 movies for you:\")\n",
    "for index, row in sim_df.iloc[:4].iterrows():\n",
    "    k = int(row['index'])\n",
    "    display(df.loc[k][1:])\n",
    "    print(70*\"-\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T20:11:18.794910300Z",
     "start_time": "2024-04-01T20:11:18.696970800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# I have some good ideas for improving this algorithm, such as using a more complex similarity computation and incorporating multiple factors for suggesting a movie, including scores and the number of voters and ...\n",
    "### This project is so fun, and I want to continue improving it for my own sake."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Branch version2:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# As you can see, I commented out the function for deleting stopwords, and the quality of the suggestions degraded. Even the movie whose plot we entered, \"Se7en,\" did not appear in our list. This illustrates that every step is important, and of course, there are some steps that I haven't implemented in this project. If I had, we might have achieved better results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
